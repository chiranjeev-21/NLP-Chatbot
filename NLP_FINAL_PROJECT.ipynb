{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wmf07eJE8p9r"
      },
      "source": [
        "# NLP PROJECT\n",
        "In this project, we are going to use the [Reformer](https://arxiv.org/abs/2001.04451), also known as the efficient Transformer, to generate a dialogue between two bots. We will feed the conversations to our model and it will learn how to understand the context of each one. Not only will it learn how to answer questions but it will also know how to ask questions if it needs more info. For example, after a customer asks for a train ticket, the chatbot can ask what time the said customer wants to leave. We can use this concept to automate call centers, hotel receptions, personal trainers, or any type of customer service.The Breakdown of this notebook is as follows:\n",
        "\n",
        "* Understand how the Reformer works\n",
        "* Explore the [MultiWoz](https://arxiv.org/abs/1810.00278) dataset\n",
        "* Process the data to feed it into the model\n",
        "* Train your model\n",
        "* Generate a dialogue by feeding a question to the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3Tvo6nMz6zt"
      },
      "source": [
        "<a name=\"1\"></a>\n",
        "# Part 1:   Exploring the MultiWoz dataset\n",
        "\n",
        "We will start by exploring the MultiWoz dataset. The dataset has more than 10,000 human annotated dialogues and spans multiple domains and topics. Some dialogues include multiple domains and others include single domains."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aV4zpTnSVFIp",
        "outputId": "e2fc1f48-0b4d-4a46-8c9d-84798e71d4e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trax                          1.4.1\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "from termcolor import colored,cprint\n",
        "\n",
        "!pip -q install trax\n",
        "import trax   \n",
        "from trax import layers as tl\n",
        "from trax.supervised import training\n",
        "!pip list | grep trax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uqFKh5pb0Lcn",
        "outputId": "cbac3500-d404-4008-fa91-e605230578b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCbyTXLsz6zx"
      },
      "outputs": [],
      "source": [
        "DATA_FILE = 'data.json'\n",
        "\n",
        "DATA_DIR = '/content/gdrive/MyDrive/week4/data'\n",
        "\n",
        "# dictionary where we load the dialogue dataset\n",
        "DIALOGUE_DB = {}\n",
        "\n",
        "# vocabulary filename\n",
        "VOCAB_FILE = 'en_32k.subword'\n",
        "\n",
        "# vocabulary file directory\n",
        "VOCAB_DIR = '/content/gdrive/MyDrive/week4/data/vocabs'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K58I5vFB7GlP"
      },
      "outputs": [],
      "source": [
        "def load_json(directory, file):\n",
        "    with open(f'{directory}/{file}') as file: \n",
        "        db = json.load(file)\n",
        "    return db\n",
        "\n",
        "DIALOGUE_DB = load_json(DATA_DIR, DATA_FILE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGBnUfEk8p9x"
      },
      "outputs": [],
      "source": [
        "print(f'The number of dialogues is: {len(DIALOGUE_DB)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE30O_Ywz6zy"
      },
      "source": [
        "The dialogues are composed of multiple files and the filenames are used as keys in our dictionary. Those with multi-domain dialogues have \"MUL\" in their filenames while single domain dialogues have either \"SNG\" or \"WOZ\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRG5N_hDz6zy"
      },
      "outputs": [],
      "source": [
        "print(list(DIALOGUE_DB.keys())[0:7]) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pE6wiMUS8p91"
      },
      "source": [
        "There are 10,438 conversations, each in its own file.  We will train our model on all those conversations. Each file is also loaded into a dictionary and each has two keys which are the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5KYeQLnG8p96"
      },
      "outputs": [],
      "source": [
        "print(DIALOGUE_DB['SNG0073.json'].keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-gj5aqF8p9_"
      },
      "source": [
        "The `goal` also points to a dictionary and it contains several keys pertaining to the objectives of the conversation. For example below, we can see that the conversation will be about booking a taxi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPPWwQ2s8p9_"
      },
      "outputs": [],
      "source": [
        "DIALOGUE_DB['SNG0073.json']['goal']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4N8RtWu8p-C"
      },
      "source": [
        "The `log` on the other hand contains the dialog. It is a list of dictionaries and each element of this list contains several descriptions as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHzrp1_hz6z0"
      },
      "outputs": [],
      "source": [
        "# get first element of the log list\n",
        "DIALOGUE_DB['SNG0073.json']['log'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sYniX-n-z6z0"
      },
      "outputs": [],
      "source": [
        "print(' Person 1: ', DIALOGUE_DB['SNG0073.json']['log'][0]['text'])\n",
        "print(' Person 2: ',DIALOGUE_DB['SNG0073.json']['log'][1]['text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRLX-s5Lz6z0"
      },
      "source": [
        "<a name=\"ex01\"></a>\n",
        "`get_conversation()` function extracts the conversations from the dataset's file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cmqukHvz6z1"
      },
      "outputs": [],
      "source": [
        "def get_conversation(file, data_db):\n",
        "    result = ''\n",
        "    len_msg_log = len(data_db[file]['log'])\n",
        "    delimiter_1 = ' Person 1: '\n",
        "    delimiter_2 = ' Person 2: '\n",
        "    \n",
        "    for i in range(len_msg_log):\n",
        "        cur_log = data_db[file]['log'][i]\n",
        "        if i%2 == 0:                   \n",
        "            result += delimiter_1\n",
        "        else: \n",
        "            result += delimiter_2\n",
        "        result += cur_log['text']\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ugvx0noP8p-G"
      },
      "outputs": [],
      "source": [
        "file = 'SNG01856.json'\n",
        "conversation = get_conversation(file, DIALOGUE_DB)\n",
        "print(conversation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rs2R8q1d8p-K"
      },
      "outputs": [],
      "source": [
        "DIALOGUE_DB['SNG01856.json']['log'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nf0AAmxy8p-N"
      },
      "source": [
        "The dataset also comes with hotel, hospital, taxi, train, police, and restaurant databases. For example, in case you need to call a doctor, or a hotel, or a taxi, this will allow you to automate the entire conversation. Take a look at the files accompanying the data set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQmYUcsi8p-O"
      },
      "outputs": [],
      "source": [
        "attraction_file = open('/content/gdrive/MyDrive/week4/data/attraction_db.json')\n",
        "attractions = json.load(attraction_file)\n",
        "print(attractions[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5kTg4uX8p-R"
      },
      "outputs": [],
      "source": [
        "hospital_file = open('/content/gdrive/MyDrive/week4/data/hospital_db.json')\n",
        "hospitals = json.load(hospital_file)\n",
        "print(hospitals[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5knaAEc8p-U"
      },
      "outputs": [],
      "source": [
        "hotel_file = open('/content/gdrive/MyDrive/week4/data/hotel_db.json')\n",
        "hotels = json.load(hotel_file)\n",
        "print(hotels[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t-Rk01Mv8p-a"
      },
      "outputs": [],
      "source": [
        "police_file = open('/content/gdrive/MyDrive/week4/data/hotel_db.json')\n",
        "police = json.load(police_file)\n",
        "print(police[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-G9pD8g8p-d"
      },
      "outputs": [],
      "source": [
        "restaurant_file = open('/content/gdrive/MyDrive/week4/data/restaurant_db.json')\n",
        "restaurants = json.load(restaurant_file)\n",
        "print(restaurants[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2H8pB_yI8p-g"
      },
      "outputs": [],
      "source": [
        "with open('/content/gdrive/MyDrive/week4/data/README') as file:\n",
        "    print(file.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IrnQ9eNV8p-k"
      },
      "outputs": [],
      "source": [
        "all_files = DIALOGUE_DB.keys()\n",
        "\n",
        "untokenized_data = []\n",
        "\n",
        "for file in all_files:\n",
        "    result = get_conversation(file, DIALOGUE_DB)\n",
        "    untokenized_data.append(result)\n",
        "print(untokenized_data[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "De0MJtQaz6z5"
      },
      "source": [
        "Now let us split the list to a train and eval dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buE0b8bjx_p_"
      },
      "outputs": [],
      "source": [
        "# shuffle the list we generated above\n",
        "random.shuffle(untokenized_data)\n",
        "\n",
        "cut_off = int(len(untokenized_data) * .05)\n",
        "\n",
        "train_data, eval_data = untokenized_data[:-cut_off], untokenized_data[-cut_off:]\n",
        "\n",
        "print(f'number of conversations in the data set: {len(untokenized_data)}')\n",
        "print(f'number of conversations in train set: {len(train_data)}')\n",
        "print(f'number of conversations in eval set: {len(eval_data)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tAAFYJOz6z6"
      },
      "source": [
        "<a name=\"2.1\"></a>\n",
        "## 2.1   Tokenizing, batching with bucketing\n",
        "We can now proceed in generating tokenized batches of our data. Let's first define a utility generator function to yield elements from our data sets:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCLq1yWZz6z6"
      },
      "outputs": [],
      "source": [
        "def stream(data):\n",
        "    # loop over the entire data\n",
        "    while True:\n",
        "        # get a random element\n",
        "        d = random.choice(data)\n",
        "        # yield a tuple pair of identical values \n",
        "        # (i.e. our inputs to the model will also be our targets during training)\n",
        "        yield (d, d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTQdGGPlz6z6"
      },
      "source": [
        "Now let's define our data pipeline for tokenizing and batching our data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZgK5FAAWwOu"
      },
      "outputs": [],
      "source": [
        "# trax allows us to use combinators to generate our data pipeline\n",
        "data_pipeline = trax.data.Serial(\n",
        "    # randomize the stream\n",
        "    trax.data.Shuffle(),\n",
        "    \n",
        "    # tokenize the data\n",
        "    trax.data.Tokenize(vocab_dir=VOCAB_DIR,\n",
        "                       vocab_file=VOCAB_FILE),\n",
        "    \n",
        "    # filter too long sequences\n",
        "    trax.data.FilterByLength(2048),\n",
        "    \n",
        "    # bucket by length\n",
        "    trax.data.BucketByLength(boundaries=[128, 256,  512, 1024],\n",
        "                             batch_sizes=[16,    8,    4,   2, 1]),\n",
        "    \n",
        "    # add loss weights but do not add it to the padding tokens (i.e. 0)\n",
        "    trax.data.AddLossWeights(id_to_mask=0)\n",
        ")\n",
        "\n",
        "# apply the data pipeline to our train and eval sets\n",
        "train_stream = data_pipeline(stream(train_data))\n",
        "eval_stream = data_pipeline(stream(eval_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iBQEvhLYRot"
      },
      "outputs": [],
      "source": [
        "# the stream generators will yield (input, target, weights). let's just grab the input for inspection\n",
        "inp, _, _ = next(train_stream)\n",
        "\n",
        "# print the shape. format is (batch size, token length)\n",
        "print(\"input shape: \", inp.shape)\n",
        "\n",
        "# detokenize the first element\n",
        "print(trax.data.detokenize(inp[0], vocab_dir=VOCAB_DIR, vocab_file=VOCAB_FILE))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMAHlg-ADYSs"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.insert(0,'/content/')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0J_0ZDc_18cL"
      },
      "source": [
        "<a name=\"3\"></a>\n",
        "# Part 3:   Reversible layers\n",
        "\n",
        "When running large deep models, we often run out of memory as each layer allocates memory to store activations for use in backpropagation. To save this resource, we need to be able to recompute these activations during the backward pass without storing them during the forward pass. Take a look first at the leftmost diagram below. \n",
        "\n",
        "This is how the residual networks are implemented in the standard Transformer. It follows that, given `F()` is Attention and `G()` is Feed-forward(FF). \n",
        ": \n",
        "\n",
        "\\begin{align}  \n",
        "\\mathrm{y}_\\mathrm{a} &= \\mathrm{x} + \\mathrm{F}\\left(\\mathrm{x}\\right)\\tag{1} \\\\\n",
        "\\mathrm{y}_{b}&=\\mathrm{y}_{a}+\\mathrm{G}\\left(\\mathrm{y}_{a}\\right)\\tag{2}\\\\\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "As you can see, it requires that $\\mathrm{x}$ and $\\mathrm{y}_{a}$ be saved so it can be used during backpropagation. We want to avoid this to conserve memory and this is where reversible residual connections come in. They are shown in the middle and rightmost diagrams above. The key idea is that we will start with two copies of the input to the model and at each layer we will only update one of them. The activations that we *don’t* update are the ones that will be used to compute the residuals. \n",
        "\n",
        "Now in this reversible set up you get the following instead: \n",
        "\n",
        "\\begin{align}  \n",
        "\\mathrm{y}_{1}&=\\mathrm{x}_{1}+\\mathrm{F}\\left(\\mathrm{x}_{2}\\right)\\tag{3}\\\\\n",
        "\\mathrm{y}_{2}&=\\mathrm{x}_{2}+\\mathrm{G}\\left(\\mathrm{y}_{1}\\right)\\tag{4}\\\\\n",
        "\\end{align}\n",
        "To recover $\\mathrm{(x_1,x_2)}$ from $\\mathrm{(y_1, y_2)}$ \n",
        "\n",
        "\\begin{align}  \n",
        "\\mathrm{x}_{2}&=\\mathrm{y}_{2}-\\mathrm{G}\\left(\\mathrm{y}_{1}\\right)\\tag{5}\\\\\n",
        "\\mathrm{x}_{1}&=\\mathrm{y}_{1}-\\mathrm{F}\\left(\\mathrm{x}_{2}\\right)\\tag{6}\\\\\n",
        "\\end{align}\n",
        "\n",
        "With this configuration, we’re now able to run the network fully in reverse. You'll notice that during the backward pass, $\\mathrm{x2}$ and $\\mathrm{x1}$ can be recomputed based solely on the values of $\\mathrm{y2}$ and $\\mathrm{y1}$. No need to save it during the forward pass."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adX2eU762BkF"
      },
      "outputs": [],
      "source": [
        "# UNQ_C2\n",
        "# GRADED FUNCTION: reversible_layer_forward\n",
        "def reversible_layer_forward(x, f, g):\n",
        "    x1, x2 = np.split(x, 2, axis=-1) \n",
        "\n",
        "    y1 = x1 + f(x2)\n",
        "    y2 = x2 + g(y1)\n",
        "    # concatenate y1 and y2 along the depth dimension. be sure output is of type np.ndarray\n",
        "    y = np.concatenate([y1, y2], axis=-1)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYGdCdAsbY0S"
      },
      "source": [
        "We will now implement the `reversible_layer_reverse` function  which is possible because at every time step you have $x_1$ and $x_2$ and $y_2$ and $y_1$, along with the function `f`, and `g`. Where `f` is the attention and `g` is the feedforward. This allows us to compute equations 5 and 6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0xTCG9WlaiiO"
      },
      "outputs": [],
      "source": [
        "def reversible_layer_reverse(y, f, g):\n",
        "    y1, y2 = np.split(y, 2, axis=-1)\n",
        "\n",
        "    x2 = y2 - g(y1)\n",
        "    x1 = y1 - f(x2)\n",
        "    # concatenate x1 and x2 along the depth dimension\n",
        "    x = np.concatenate([x1, x2], axis=-1)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RidbAcoR6duP"
      },
      "outputs": [],
      "source": [
        "#Reformer Language Model\n",
        "def ReformerLM(vocab_size=33000, n_layers=2, mode='train', attention_type=tl.SelfAttention):\n",
        "    model = tl.Serial( \n",
        "                trax.models.reformer.ReformerLM( \n",
        "                vocab_size=vocab_size,\n",
        "                n_layers=n_layers,\n",
        "                mode=mode,\n",
        "                attention_type=attention_type\n",
        "            ), tl.LogSoftmax() \n",
        "        )\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYWRIgQRz6z_"
      },
      "outputs": [],
      "source": [
        "temp_model = ReformerLM('train')\n",
        "print(str(temp_model))\n",
        "del temp_model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BS-bdS2Rz6z_"
      },
      "source": [
        "We will now use CrossEntropyLoss loss function with Adam optimizer to optimize our network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tQehGhoD4Psl"
      },
      "outputs": [],
      "source": [
        "def training_loop(ReformerLM, train_gen, eval_gen, output_dir = \"./model/\"):\n",
        "    # use the warmup_and_rsqrt_decay learning rate schedule\n",
        "    lr_schedule = trax.lr.warmup_and_rsqrt_decay(\n",
        "        n_warmup_steps=1000, max_value=0.01)\n",
        "\n",
        "    train_task = training.TrainTask(            \n",
        "        train_gen,\n",
        "        tl.CrossEntropyLoss(),\n",
        "        trax.optimizers.Adam(0.01),\n",
        "        lr_schedule,\n",
        "        None)\n",
        "\n",
        "    eval_task = training.EvalTask(                      \n",
        "        labeled_data=eval_gen,\n",
        "        metrics=[tl.CrossEntropyLoss(), tl.Accuracy()]\n",
        "    )\n",
        "\n",
        "    loop = training.Loop(ReformerLM(mode='train'),\n",
        "                         train_task,\n",
        "                         eval_tasks=[eval_task],\n",
        "                         output_dir=output_dir)\n",
        "    return loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IGCDSBeTz60A",
        "outputId": "452bd15f-d064-46ce-95dc-a68b6fb64146"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/jax/_src/xla_bridge.py:658: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[<trax.supervised.training.TrainTask object at 0x7fb3600cf7c0>]\n",
            "[<trax.supervised.training.EvalTask object at 0x7fb36006cc10>]\n"
          ]
        }
      ],
      "source": [
        "# # UNIT TEST COMMENT: Use the train task and eval task for grading train_model\n",
        "# test_loop = training_loop(ReformerLM, train_stream, eval_stream)\n",
        "# train_task = test_loop.tasks\n",
        "# eval_task = test_loop.eval_tasks\n",
        "\n",
        "# print(train_task)\n",
        "# print(eval_task)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0t7-kHxz60A",
        "outputId": "1cc7733c-1270-4a4f-e544-bcedb5076dc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[92m All tests passed\n"
          ]
        }
      ],
      "source": [
        "# # BEGIN UNIT TEST\n",
        "# # w4_unittest.test_tasks(train_task, eval_task)\n",
        "# w4_unittest.test_tasks(test_loop)\n",
        "# # END UNIT TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9jERXY46I6J",
        "outputId": "ff6c2414-4094-45ae-8b5d-2d0fcf4d3de1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/jax/_src/xla_bridge.py:658: UserWarning: jax.host_count has been renamed to jax.process_count. This alias will eventually be removed; please update your code.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# !rm -f model/model.pkl.gz\n",
        "# loop = training_loop(ReformerLM, train_stream, eval_stream)\n",
        "# loop.run(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H-fepo1z60B"
      },
      "source": [
        "**Approximate Expected output:**  \n",
        "\n",
        "```\n",
        "\n",
        "Step      1: Ran 1 train steps in 55.73 secs\n",
        "Step      1: train CrossEntropyLoss |  10.41907787\n",
        "Step      1: eval  CrossEntropyLoss |  10.41005802\n",
        "Step      1: eval          Accuracy |  0.00000000\n",
        "\n",
        "Step     10: Ran 9 train steps in 108.21 secs\n",
        "Step     10: train CrossEntropyLoss |  10.15449715\n",
        "Step     10: eval  CrossEntropyLoss |  9.63478279\n",
        "Step     10: eval          Accuracy |  0.16350447\n",
        "``` "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAGlkMZW6rKn"
      },
      "source": [
        "We will be using the [autoregressive_sample_stream()](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.decoding.autoregressive_sample_stream) decoding method from Trax to do fast inference. Let's define a few parameters to initialize our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3L-hIi3z60B"
      },
      "outputs": [],
      "source": [
        "# define the `predict_mem_len` and `predict_drop_len` of tl.SelfAttention\n",
        "def attention(*args, **kwargs):\n",
        "    # number of input positions to remember in a cache when doing fast inference. \n",
        "    kwargs['predict_mem_len'] = 120\n",
        "    # number of input elements to drop once the fast inference input cache fills up.\n",
        "    kwargs['predict_drop_len'] = 120\n",
        "    # return the attention layer with the parameters defined above\n",
        "    return tl.SelfAttention(*args, **kwargs)\n",
        "\n",
        "# define the model using the ReformerLM function you implemented earlier.\n",
        "model = ReformerLM(\n",
        "    vocab_size=33000,\n",
        "    n_layers=6,\n",
        "    mode='predict',\n",
        "    attention_type=attention,\n",
        ")\n",
        "\n",
        "# define an input signature so we can initialize our model. shape will be (1, 1) and the data type is int32.\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARynlJJ2z60B"
      },
      "source": [
        "We can now initialize our model from a file containing the pretrained weights. We will save this starting state so we can reset the model state when we generate a new conversation. This will become clearer in the `generate_dialogue()` function later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CEyA6irz60B"
      },
      "outputs": [],
      "source": [
        "# initialize from file\n",
        "model.init_from_file('/content/gdrive/MyDrive/week4/chatbot_model1.pkl.gz',weights_only=True, input_signature=shape11)\n",
        "\n",
        "# save the starting state\n",
        "STARTING_STATE = model.state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQG-Y8Rpz60B"
      },
      "source": [
        "Let's define a few utility functions as well to help us tokenize and detokenize. We can use the [tokenize()](https://trax-ml.readthedocs.io/en/latest/trax.data.html#trax.data.tf_inputs.tokenize) and [detokenize()](https://trax-ml.readthedocs.io/en/latest/trax.data.html#trax.data.tf_inputs.detokenize) from `trax.data.tf_inputs` to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOm4UxCJz60C"
      },
      "outputs": [],
      "source": [
        "def tokenize(sentence, vocab_file, vocab_dir):\n",
        "    return list(trax.data.tokenize(iter([sentence]), vocab_file=vocab_file, vocab_dir=vocab_dir))[0]\n",
        "\n",
        "def detokenize(tokens, vocab_file, vocab_dir):\n",
        "    return trax.data.detokenize(tokens, vocab_file=vocab_file, vocab_dir=vocab_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQgQA8RTz60C"
      },
      "source": [
        "We are now ready to define our decoding function. This will return a generator that yields that next symbol output by the model. It will be able to predict the next words by just feeding it a starting sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1wdWgdZz60C"
      },
      "source": [
        "<a name=\"ex06\"></a>\n",
        "### Exercise 06\n",
        "**Instructions:** Implement the function below to return a generator that predicts the next word of the conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nr493M-Ez60C"
      },
      "outputs": [],
      "source": [
        "# UNQ_C6\n",
        "# GRADED FUNCTION\n",
        "def ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file, vocab_dir, temperature):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        ReformerLM:  the Reformer language model you just trained\n",
        "        start_sentence (string): starting sentence of the conversation\n",
        "        vocab_file (string): vocabulary filename\n",
        "        vocab_dir (string): directory of the vocabulary file\n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "\n",
        "    Returns:\n",
        "        generator: yields the next symbol generated by the model\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE (REPLACE INSTANCES OF 'None' WITH YOUR CODE) ###\n",
        "    \n",
        "    # Create input tokens using the the tokenize function\n",
        "    input_tokens = tokenize(start_sentence, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
        "    \n",
        "    # Add batch dimension to array. Convert from (n,) to (x, n) where \n",
        "    # x is the batch size. Default is 1. (hint: you can use np.expand_dims() with axis=0)\n",
        "    # input_tokens_with_batch = np.array(input_tokens)[None, :]\n",
        "    input_tokens_with_batch = np.expand_dims(input_tokens, axis=0)\n",
        "    \n",
        "    # call the autoregressive_sample_stream function from trax\n",
        "    output_gen = trax.supervised.decoding.autoregressive_sample_stream( \n",
        "        # model\n",
        "        model=ReformerLM,\n",
        "        # inputs will be the tokens with batch dimension\n",
        "        inputs=input_tokens_with_batch,\n",
        "        # temperature\n",
        "        temperature=temperature\n",
        "    )\n",
        "    \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return output_gen"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IariUOiuz60C",
        "outputId": "a7cd9c7a-e466-46fb-9717-e036f93c57dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Array(1, dtype=int32), Array(0, dtype=int32), Array(4, dtype=int32), Array(3, dtype=int32), Array(0, dtype=int32), Array(4, dtype=int32)]\n"
          ]
        }
      ],
      "source": [
        "# BEGIN UNIT TEST\n",
        "import pickle\n",
        "\n",
        "WEIGHTS_FROM_FILE = ()\n",
        "\n",
        "with open('weights', 'rb') as file:\n",
        "    WEIGHTS_FROM_FILE = pickle.load(file)\n",
        "\n",
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
        "\n",
        "def attention(*args, **kwargs):\n",
        "    kwargs['predict_mem_len'] = 120\n",
        "    kwargs['predict_drop_len'] = 120\n",
        "    return tl.SelfAttention(*args, **kwargs)\n",
        "\n",
        "test_model = ReformerLM(vocab_size=5, n_layers=1, mode='predict', attention_type=attention)\n",
        "\n",
        "test_output_gen = ReformerLM_output_gen(test_model, \"test\", vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=0)\n",
        "\n",
        "test_model.init_weights_and_state(shape11)\n",
        "\n",
        "test_model.weights = WEIGHTS_FROM_FILE\n",
        "\n",
        "output = []\n",
        "\n",
        "for i in range(6):\n",
        "    output.append(next(test_output_gen)[0])\n",
        "\n",
        "print(output)\n",
        "\n",
        "# free memory\n",
        "del test_model \n",
        "del WEIGHTS_FROM_FILE\n",
        "del test_output_gen\n",
        "# END UNIT TEST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_G1lO3gDhpr",
        "outputId": "f37a2510-a394-4bdc-83c1-ec1c5b6e072a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated output [Array(1, dtype=int32), Array(0, dtype=int32), Array(4, dtype=int32), Array(3, dtype=int32), Array(0, dtype=int32), Array(4, dtype=int32)]\n",
            "\u001b[92m All tests passed\n"
          ]
        }
      ],
      "source": [
        "test_model = ReformerLM(vocab_size=5, n_layers=1, mode='predict', attention_type=attention)\n",
        "test_output_gen = ReformerLM_output_gen(test_model, \"test\", vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=0)\n",
        "\n",
        "w4_unittest.test_ReformerLM_output_gen(test_model, test_output_gen)\n",
        "del test_model, test_output_gen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zR-GEYzAz60D"
      },
      "source": [
        "***Expected value:***\n",
        "\n",
        "[1, 0, 4, 3, 0, 4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ws29J3S9z60D"
      },
      "source": [
        "Great! Now you will be able to see the model in action. The utility function below will call the generator you just implemented and will just format the output to be easier to read. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KT5YBO-Hz60D"
      },
      "outputs": [],
      "source": [
        "shape11 = trax.shapes.ShapeDtype((1, 1), dtype=np.int32)\n",
        "\n",
        "def attention(*args, **kwargs):\n",
        "    kwargs['predict_mem_len'] = 120  # max length for predictions\n",
        "    kwargs['predict_drop_len'] = 120  # never drop old stuff\n",
        "    return tl.SelfAttention(*args, **kwargs)\n",
        "\n",
        "model = ReformerLM(\n",
        "    vocab_size=33000,\n",
        "    n_layers=6,\n",
        "    mode='predict',\n",
        "    attention_type=attention,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MFO1iwt3z60D"
      },
      "outputs": [],
      "source": [
        "model.init_from_file('/content/gdrive/MyDrive/week4/chatbot_model1.pkl.gz',\n",
        "                     weights_only=True, input_signature=shape11)\n",
        "\n",
        "STARTING_STATE = model.state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5D8SY4QRz60D"
      },
      "outputs": [],
      "source": [
        "# def generate_dialogue(ReformerLM, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature):\n",
        "#     \"\"\"\n",
        "#     Args:\n",
        "#         ReformerLM:  the Reformer language model you just trained\n",
        "#         model_state (np.array): initial state of the model before decoding\n",
        "#         start_sentence (string): starting sentence of the conversation\n",
        "#         vocab_file (string): vocabulary filename\n",
        "#         vocab_dir (string): directory of the vocabulary file\n",
        "#         max_len (int): maximum number of tokens to generate \n",
        "#         temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "#             0.0: same as argmax, always pick the most probable token\n",
        "#             1.0: sampling from the distribution (can sometimes say random things)\n",
        "\n",
        "#     Returns:\n",
        "#         generator: yields the next symbol generated by the model\n",
        "#     \"\"\"  \n",
        "    \n",
        "#     # define the delimiters we used during training\n",
        "#     delimiter_1 = 'Person 1: ' \n",
        "#     delimiter_2 = 'Person 2: '\n",
        "    \n",
        "#     # initialize detokenized output\n",
        "#     sentence = ''\n",
        "    \n",
        "#     # token counter\n",
        "#     counter = 0\n",
        "    \n",
        "#     # output tokens. we insert a ': ' for formatting\n",
        "#     result = [tokenize(': ', vocab_file=vocab_file, vocab_dir=vocab_dir)]\n",
        "    \n",
        "#     # reset the model state when starting a new dialogue\n",
        "#     ReformerLM.state = model_state\n",
        "    \n",
        "#     # calls the output generator implemented earlier\n",
        "#     output = ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)\n",
        "    \n",
        "#     # print the starting sentence\n",
        "#     print(start_sentence.split(delimiter_2)[0].strip())\n",
        "    \n",
        "#     # loop below yields the next tokens until max_len is reached. the if-elif is just for prettifying the output.\n",
        "#     for o in output:\n",
        "        \n",
        "#         result.append(o)\n",
        "        \n",
        "#         sentence = detokenize(np.concatenate(result, axis=0), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)\n",
        "        \n",
        "#         if sentence.endswith(delimiter_1):\n",
        "#             sentence = sentence.split(delimiter_1)[0]\n",
        "#             print(f'{delimiter_2}{sentence}')\n",
        "#             sentence = ''\n",
        "#             result.clear()\n",
        "        \n",
        "#         elif sentence.endswith(delimiter_2):\n",
        "#             sentence = sentence.split(delimiter_2)[0]\n",
        "#             print(f'{delimiter_1}{sentence}')\n",
        "#             sentence = ''\n",
        "#             result.clear()\n",
        "\n",
        "#         counter += 1\n",
        "        \n",
        "#         if counter > max_len:\n",
        "#             break    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Vok0AJuDz5T"
      },
      "outputs": [],
      "source": [
        "def generate_dialogue(ReformerLM, model_state, start_sentence, vocab_file, vocab_dir, max_len, temperature):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        ReformerLM:  the Reformer language model you just trained\n",
        "        model_state (np.array): initial state of the model before decoding\n",
        "        start_sentence (string): starting sentence of the conversation\n",
        "        vocab_file (string): vocabulary filename\n",
        "        vocab_dir (string): directory of the vocabulary file\n",
        "        max_len (int): maximum number of tokens to generate \n",
        "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
        "            0.0: same as argmax, always pick the most probable token\n",
        "            1.0: sampling from the distribution (can sometimes say random things)\n",
        "\n",
        "    Returns:\n",
        "        generator: yields the next symbol generated by the model\n",
        "    \"\"\"  \n",
        "    \n",
        "    # define the delimiters we used during training\n",
        "    delimiter_1 = 'Person 1: ' \n",
        "    delimiter_2 = 'Person 2: '\n",
        "    \n",
        "    # initialize detokenized output\n",
        "    sentence = ''\n",
        "    \n",
        "    # token counter\n",
        "    counter = 0\n",
        "    \n",
        "    # output tokens. we insert a ': ' for formatting\n",
        "    result = [tokenize(': ', vocab_file=vocab_file, vocab_dir=vocab_dir)]\n",
        "    \n",
        "    # reset the model state when starting a new dialogue\n",
        "    ReformerLM.state = model_state\n",
        "    \n",
        "    # calls the output generator implemented earlier\n",
        "    output = ReformerLM_output_gen(ReformerLM, start_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, temperature=temperature)\n",
        "    \n",
        "    # print the starting sentence\n",
        "    print(start_sentence.split(delimiter_2)[0].strip())\n",
        "    \n",
        "    # loop below yields the next tokens until max_len is reached. the if-elif is just for prettifying the output.\n",
        "    for o in output:\n",
        "        \n",
        "        result.append(o)\n",
        "        \n",
        "        sentence = detokenize(np.concatenate(result, axis=0), vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)\n",
        "        \n",
        "        if sentence.endswith(delimiter_1):\n",
        "            sentence = sentence.split(delimiter_1)[0]\n",
        "            print(f'{delimiter_2}{sentence}')\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "        \n",
        "        elif sentence.endswith(delimiter_2):\n",
        "            sentence = sentence.split(delimiter_2)[0]\n",
        "            print(f'{delimiter_1}{sentence}')\n",
        "            sentence = ''\n",
        "            result.clear()\n",
        "\n",
        "        counter += 1\n",
        "        \n",
        "        if counter > max_len:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mouSZNKGz60E"
      },
      "source": [
        "We can now feed in different starting sentences and see how the model generates the dialogue. You can even input your own starting sentence. Just remember to ask a question that covers the topics in the Multiwoz dataset so you can generate a meaningful conversation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvjrFRmrz60E",
        "outputId": "2c40f5e6-bff5-4af7-b44c-0cbcf666160b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Person 1: Are there theatres in town?\n",
            "Person 2: : There are 4 theatres in town. Do you have a preference on area? \n",
            "Person 1: No, I don't care. Which one do you recommend? \n",
            "Person 2: I recommend the Mumford Theatre. It's in the east at Anglia Ruskin Enterprise, east road. Would you like more information on it? \n",
            "Person 1: Yes, could I have the postcode, and entrance fee please? \n",
            "Person 1: The postcode is cb11pt then i hate i hate i hate i hate i need their info \n"
          ]
        }
      ],
      "source": [
        "sample_sentence = ' Person 1: Are there theatres in town? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l0gyLryfz60E",
        "outputId": "bcbb942a-24a0-4ddd-b672-730b6739cffd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Person 1: Is there a hospital nearby?\n",
            "Person 2: : Addensbrookes Hospital is located at Hills Rd, Cambridge, postcode CB20QQ. \n",
            "Person 1: Thank you for the information. That's all I need. \n",
            "Person 2: Thank you for using our services.Goodbye.\n",
            "Person 1: Thank you for your help. \n",
            "Person 2: Thank you for using our services.Goodbye.\n",
            "Person 1: Goodbye. \n"
          ]
        }
      ],
      "source": [
        "sample_sentence = ' Person 1: Is there a hospital nearby? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uXEjRsQz60E",
        "outputId": "ecd76fec-fce0-4a54-acab-f6e5996232fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Person 1: Can you book a taxi?\n",
            "Person 2: : I sure can. When would you like to leave? \n",
            "Person 1: I want to leave after 13:00. \n",
            "Person 2: I'm sorry, I have no listings for that time. Would you like to try a different time? \n",
            "Person 1: Yes, let's try to find a train. \n",
            "Person 2: I'm sorry, but I'm not able to book that train. Could you change your request? \n"
          ]
        }
      ],
      "source": [
        "sample_sentence = ' Person 1: Can you book a taxi? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aC9-TwdEKue",
        "outputId": "a29f3b3a-2080-44d6-bf68-91022e387bfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Person 1: Where is the Japanese restaurant?\n",
            "Person 2: : There is no such listing for the restaurant called Cote. Would you like to book a table? \n",
            "Person 1: Yes, I would like to book a table for 1 at 18:00 on Friday. \n",
            "Person 2: I'm sorry but I am unable to book that for you. Would you like to try another time or day perhaps another day perhaps? \n",
            "Person 1: How about 18:00? \n",
            "Person 2: I was able to book you at Cote. Your reference is  \n"
          ]
        }
      ],
      "source": [
        "sample_sentence = ' Person 1: Where is the Japanese restaurant? Person 2: '\n",
        "generate_dialogue(ReformerLM=model, model_state=STARTING_STATE, start_sentence=sample_sentence, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR, max_len=120, temperature=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xb2OesfiGMHh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "coursera": {
      "schema_names": [
        "NLPC4-4"
      ]
    },
    "gpuClass": "standard",
    "jupytext": {
      "encoding": "# -*- coding: utf-8 -*-",
      "formats": "ipynb,py:percent",
      "text_representation": {
        "extension": ".py",
        "format_name": "percent",
        "format_version": "1.3",
        "jupytext_version": "1.5.2"
      }
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}